{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Country with Client IP\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "At this stage, we are loading the data into a dataframe and then dropping the columns that are not needed for the model. Looking at the code for how IPs are generated, we can see that the first 3 octets are deterministic while the last octet is completely random. Therefore, we can use the first 3 octets to predict the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country  ip1  ip2  ip3\n",
      "0       21  113   98   85\n",
      "1       60   95  222   97\n",
      "2       60   95  222   97\n",
      "3      153  244  222  137\n",
      "4      153  244  222  137\n"
     ]
    }
   ],
   "source": [
    "# load request.csv file into a dataframe\n",
    "df = pd.read_csv('request.csv', on_bad_lines='skip')\n",
    "\n",
    "# drop columns 0,1,2,3,6,7,8\n",
    "df.drop(df.columns[[0,1,2,3,6,7,8]], axis=1, inplace=True)\n",
    "\n",
    "# add headers to dataframe; first column is 'country', second column is 'ip'\n",
    "df.columns = ['country', 'ip']\n",
    "\n",
    "# split ip into 4 columns\n",
    "df[['ip1','ip2','ip3','ip4']] = df.ip.str.split(\".\",expand=True,)\n",
    "\n",
    "# drop ip column\n",
    "df.drop(['ip'], axis=1, inplace=True)\n",
    "\n",
    "# drop ip4 column\n",
    "df.drop(['ip4'], axis=1, inplace=True)\n",
    "\n",
    "# encode labels\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# encode country column\n",
    "df['country'] = encoder.fit_transform(df['country'])\n",
    "\n",
    "# print first 5 rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Selection\n",
    "\n",
    "We split into train and test sets. Our goal is to predict the country give an ip. We will try a few models and see which one performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# split train and test sets into X and y\n",
    "X_train = train.drop('country', axis=1)\n",
    "y_train = train['country']\n",
    "\n",
    "X_test = test.drop('country', axis=1)\n",
    "y_test = test['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression trained.\n",
      "Accuracy: 0.08902691511387163\n",
      "\n",
      "random_forest trained.\n",
      "Accuracy: 1.0\n",
      "\n",
      "decision_tree trained.\n",
      "Accuracy: 1.0\n",
      "\n",
      "knn trained.\n",
      "Accuracy: 1.0\n",
      "\n",
      "gaussian_nb trained.\n",
      "Accuracy: 0.20956420744331666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# try different models which are good for classification\n",
    "models = {\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'gaussian_nb': GaussianNB(),\n",
    "}\n",
    "\n",
    "# train and test models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(name + ' trained.')\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Evaluation\n",
    "\n",
    "Random Forest, Decision Tree, and KNN are the most accurate models, with an accuracy of 1. So, let's properly build a Random Forest model and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 96   0   0 ...   0   0   0]\n",
      " [  0 109   0 ...   0   0   0]\n",
      " [  0   0 100 ...   0   0   0]\n",
      " ...\n",
      " [  0   0   0 ...  93   0   0]\n",
      " [  0   0   0 ...   0 105   0]\n",
      " [  0   0   0 ...   0   0 101]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        96\n",
      "           1       1.00      1.00      1.00       109\n",
      "           2       1.00      1.00      1.00       100\n",
      "           3       1.00      1.00      1.00        88\n",
      "           4       1.00      1.00      1.00       106\n",
      "           5       1.00      1.00      1.00        96\n",
      "           6       1.00      1.00      1.00       105\n",
      "           7       1.00      1.00      1.00        94\n",
      "           8       1.00      1.00      1.00        95\n",
      "           9       1.00      1.00      1.00       102\n",
      "          10       1.00      1.00      1.00       101\n",
      "          11       1.00      1.00      1.00       104\n",
      "          12       1.00      1.00      1.00        84\n",
      "          13       1.00      1.00      1.00       105\n",
      "          14       1.00      1.00      1.00        91\n",
      "          15       1.00      1.00      1.00       102\n",
      "          16       1.00      1.00      1.00       114\n",
      "          17       1.00      1.00      1.00       103\n",
      "          18       1.00      1.00      1.00       107\n",
      "          19       1.00      1.00      1.00        94\n",
      "          20       1.00      1.00      1.00       101\n",
      "          21       1.00      1.00      1.00       109\n",
      "          22       1.00      1.00      1.00       103\n",
      "          23       1.00      1.00      1.00        95\n",
      "          24       1.00      1.00      1.00        96\n",
      "          25       1.00      1.00      1.00        92\n",
      "          26       1.00      1.00      1.00       103\n",
      "          27       1.00      1.00      1.00        99\n",
      "          28       1.00      1.00      1.00        92\n",
      "          29       1.00      1.00      1.00       100\n",
      "          30       1.00      1.00      1.00       101\n",
      "          31       1.00      1.00      1.00        85\n",
      "          32       1.00      1.00      1.00       114\n",
      "          33       1.00      1.00      1.00       100\n",
      "          34       1.00      1.00      1.00       109\n",
      "          35       1.00      1.00      1.00       103\n",
      "          36       1.00      1.00      1.00        91\n",
      "          37       1.00      1.00      1.00        84\n",
      "          38       1.00      1.00      1.00       104\n",
      "          39       1.00      1.00      1.00        87\n",
      "          40       1.00      1.00      1.00        96\n",
      "          41       1.00      1.00      1.00        86\n",
      "          42       1.00      1.00      1.00        97\n",
      "          43       1.00      1.00      1.00       100\n",
      "          44       1.00      1.00      1.00       100\n",
      "          45       1.00      1.00      1.00       118\n",
      "          46       1.00      1.00      1.00        88\n",
      "          47       1.00      1.00      1.00       109\n",
      "          48       1.00      1.00      1.00        96\n",
      "          49       1.00      1.00      1.00       103\n",
      "          50       1.00      1.00      1.00       112\n",
      "          51       1.00      1.00      1.00        91\n",
      "          52       1.00      1.00      1.00        96\n",
      "          53       1.00      1.00      1.00       100\n",
      "          54       1.00      1.00      1.00       107\n",
      "          55       1.00      1.00      1.00       109\n",
      "          56       1.00      1.00      1.00        97\n",
      "          57       1.00      1.00      1.00        93\n",
      "          58       1.00      1.00      1.00       122\n",
      "          59       1.00      1.00      1.00        92\n",
      "          60       1.00      1.00      1.00       108\n",
      "          61       1.00      1.00      1.00       100\n",
      "          62       1.00      1.00      1.00       100\n",
      "          63       1.00      1.00      1.00       103\n",
      "          64       1.00      1.00      1.00        95\n",
      "          65       1.00      1.00      1.00       107\n",
      "          66       1.00      1.00      1.00       103\n",
      "          67       1.00      1.00      1.00        81\n",
      "          68       1.00      1.00      1.00        89\n",
      "          69       1.00      1.00      1.00        98\n",
      "          70       1.00      1.00      1.00       103\n",
      "          71       1.00      1.00      1.00        97\n",
      "          72       1.00      1.00      1.00        87\n",
      "          73       1.00      1.00      1.00       114\n",
      "          74       1.00      1.00      1.00        98\n",
      "          75       1.00      1.00      1.00       105\n",
      "          76       1.00      1.00      1.00        95\n",
      "          77       1.00      1.00      1.00        77\n",
      "          78       1.00      1.00      1.00        93\n",
      "          79       1.00      1.00      1.00       108\n",
      "          80       1.00      1.00      1.00       104\n",
      "          81       1.00      1.00      1.00        95\n",
      "          82       1.00      1.00      1.00        89\n",
      "          83       1.00      1.00      1.00       103\n",
      "          84       1.00      1.00      1.00       100\n",
      "          85       1.00      1.00      1.00       101\n",
      "          86       1.00      1.00      1.00       106\n",
      "          87       1.00      1.00      1.00       110\n",
      "          88       1.00      1.00      1.00       111\n",
      "          89       1.00      1.00      1.00        97\n",
      "          90       1.00      1.00      1.00       125\n",
      "          91       1.00      1.00      1.00       109\n",
      "          92       1.00      1.00      1.00       112\n",
      "          93       1.00      1.00      1.00       115\n",
      "          94       1.00      1.00      1.00        89\n",
      "          95       1.00      1.00      1.00       111\n",
      "          96       1.00      1.00      1.00        77\n",
      "          97       1.00      1.00      1.00        98\n",
      "          98       1.00      1.00      1.00       108\n",
      "          99       1.00      1.00      1.00       111\n",
      "         100       1.00      1.00      1.00       107\n",
      "         101       1.00      1.00      1.00        85\n",
      "         102       1.00      1.00      1.00       108\n",
      "         103       1.00      1.00      1.00       107\n",
      "         104       1.00      1.00      1.00       114\n",
      "         105       1.00      1.00      1.00       118\n",
      "         106       1.00      1.00      1.00        90\n",
      "         107       1.00      1.00      1.00        98\n",
      "         108       1.00      1.00      1.00        91\n",
      "         109       1.00      1.00      1.00       118\n",
      "         110       1.00      1.00      1.00        80\n",
      "         111       1.00      1.00      1.00       109\n",
      "         112       1.00      1.00      1.00        91\n",
      "         113       1.00      1.00      1.00        99\n",
      "         114       1.00      1.00      1.00        95\n",
      "         115       1.00      1.00      1.00       118\n",
      "         116       1.00      1.00      1.00        94\n",
      "         117       1.00      1.00      1.00        89\n",
      "         118       1.00      1.00      1.00        94\n",
      "         119       1.00      1.00      1.00       120\n",
      "         120       1.00      1.00      1.00       132\n",
      "         121       1.00      1.00      1.00        97\n",
      "         122       1.00      1.00      1.00       103\n",
      "         123       1.00      1.00      1.00       129\n",
      "         124       1.00      1.00      1.00       100\n",
      "         125       1.00      1.00      1.00       131\n",
      "         126       1.00      1.00      1.00       114\n",
      "         127       1.00      1.00      1.00        94\n",
      "         128       1.00      1.00      1.00       104\n",
      "         129       1.00      1.00      1.00       115\n",
      "         130       1.00      1.00      1.00       104\n",
      "         131       1.00      1.00      1.00        99\n",
      "         132       1.00      1.00      1.00        81\n",
      "         133       1.00      1.00      1.00       108\n",
      "         134       1.00      1.00      1.00        95\n",
      "         135       1.00      1.00      1.00        98\n",
      "         136       1.00      1.00      1.00       115\n",
      "         137       1.00      1.00      1.00       108\n",
      "         138       1.00      1.00      1.00       117\n",
      "         139       1.00      1.00      1.00       103\n",
      "         140       1.00      1.00      1.00        93\n",
      "         141       1.00      1.00      1.00       108\n",
      "         142       1.00      1.00      1.00       115\n",
      "         143       1.00      1.00      1.00        98\n",
      "         144       1.00      1.00      1.00        92\n",
      "         145       1.00      1.00      1.00        99\n",
      "         146       1.00      1.00      1.00       101\n",
      "         147       1.00      1.00      1.00       111\n",
      "         148       1.00      1.00      1.00        96\n",
      "         149       1.00      1.00      1.00       112\n",
      "         150       1.00      1.00      1.00       107\n",
      "         151       1.00      1.00      1.00        96\n",
      "         152       1.00      1.00      1.00        94\n",
      "         153       1.00      1.00      1.00       110\n",
      "         154       1.00      1.00      1.00       113\n",
      "         155       1.00      1.00      1.00       108\n",
      "         156       1.00      1.00      1.00        90\n",
      "         157       1.00      1.00      1.00       111\n",
      "         158       1.00      1.00      1.00       109\n",
      "         159       1.00      1.00      1.00        95\n",
      "         160       1.00      1.00      1.00       109\n",
      "         161       1.00      1.00      1.00       108\n",
      "         162       1.00      1.00      1.00        89\n",
      "         163       1.00      1.00      1.00        98\n",
      "         164       1.00      1.00      1.00       113\n",
      "         165       1.00      1.00      1.00       101\n",
      "         166       1.00      1.00      1.00       103\n",
      "         167       1.00      1.00      1.00       115\n",
      "         168       1.00      1.00      1.00       112\n",
      "         169       1.00      1.00      1.00        99\n",
      "         170       1.00      1.00      1.00       109\n",
      "         171       1.00      1.00      1.00        98\n",
      "         172       1.00      1.00      1.00       108\n",
      "         173       1.00      1.00      1.00        90\n",
      "         174       1.00      1.00      1.00        96\n",
      "         175       1.00      1.00      1.00       106\n",
      "         176       1.00      1.00      1.00       118\n",
      "         177       1.00      1.00      1.00        93\n",
      "         178       1.00      1.00      1.00        86\n",
      "         179       1.00      1.00      1.00        95\n",
      "         180       1.00      1.00      1.00       115\n",
      "         181       1.00      1.00      1.00       104\n",
      "         182       1.00      1.00      1.00       103\n",
      "         183       1.00      1.00      1.00       109\n",
      "         184       1.00      1.00      1.00       107\n",
      "         185       1.00      1.00      1.00       102\n",
      "         186       1.00      1.00      1.00       104\n",
      "         187       1.00      1.00      1.00       106\n",
      "         188       1.00      1.00      1.00        90\n",
      "         189       1.00      1.00      1.00        97\n",
      "         190       1.00      1.00      1.00        90\n",
      "         191       1.00      1.00      1.00        93\n",
      "         192       1.00      1.00      1.00        93\n",
      "         193       1.00      1.00      1.00       105\n",
      "         194       1.00      1.00      1.00       101\n",
      "\n",
      "    accuracy                           1.00     19803\n",
      "   macro avg       1.00      1.00      1.00     19803\n",
      "weighted avg       1.00      1.00      1.00     19803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# train model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# print accuracy, confusion matrix, and classification report\n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "print()\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Income\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "Looking at the code, we can see that the gender, age, income, time, and ip are all generated randomly. Let's see if we can predict the income given the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   file  country  gender  age  income\n",
      "0  3475       21       0    6       7\n",
      "1  4678       60       1    5       7\n",
      "2  4678       60       1    5       7\n",
      "3  2116      153       1    5       3\n",
      "4  2116      153       1    5       3\n"
     ]
    }
   ],
   "source": [
    "def process_income_data():\n",
    "  # load request.csv file into a dataframe\n",
    "  df = pd.read_csv('request.csv', on_bad_lines='skip')\n",
    "\n",
    "  # remove column 1,3,5 \n",
    "  df.drop(df.columns[[0,1,3,5]], axis=1, inplace=True)\n",
    "\n",
    "  # add headers to dataframe: id, time, file, country, gender, age, income\n",
    "  df.columns = ['file', 'country', 'gender', 'age', 'income']\n",
    "\n",
    "  # turn file column from 'html/1.html' to '1'\n",
    "  df['file'] = df['file'].str.replace('html/', '').str.replace('.html', '')\n",
    "\n",
    "  # turn into boolean values\n",
    "  df['gender'] = df['gender'].map({'Male': 1, 'Female': 0})\n",
    "\n",
    "  list_of_ages = ['0-16', '17-25', '26-35', '36-45', '46-55', '56-65', '66-75', '76+']\n",
    "  list_of_incomes = ['0-10k', '10k-20k', '20k-40k', '40k-60k', '60k-100k', '100k-150k', '150k-250k', '250k+']\n",
    "\n",
    "  # turn age into integer values\n",
    "  df['age'] = df['age'].map({'0-16': 0, '17-25': 1, '26-35': 2, '36-45': 3, '46-55': 4, '56-65': 5, '66-75': 6, '76+': 7})\n",
    "\n",
    "  # turn income into integer values\n",
    "  df['income'] = df['income'].map({'0-10k': 0, '10k-20k': 1, '20k-40k': 2, '40k-60k': 3, '60k-100k': 4, '100k-150k': 5, '150k-250k': 6, '250k+': 7})\n",
    "\n",
    "  # encode country labels\n",
    "  encoder = LabelEncoder()\n",
    "\n",
    "  # encode country column\n",
    "  df['country'] = encoder.fit_transform(df['country'])\n",
    "\n",
    "  # print first 5 rows\n",
    "  print(df.head())\n",
    "\n",
    "  return df\n",
    "\n",
    "df = process_income_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model Selection\n",
    "\n",
    "Let's try a few different classification models and see which one performs better. We're going to make a test/train split and then fit the models on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_test_split(df):\n",
    "    # create test train split\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "    # split train and test sets into X and y; we are attempting to guess income\n",
    "    X_train = train.drop('income', axis=1)\n",
    "    y_train = train['income']\n",
    "\n",
    "    X_test = test.drop('income', axis=1)\n",
    "    y_test = test['income']\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_regression trained.\n",
      "Accuracy: 0.12659698025551683\n",
      "\n",
      "random_forest trained.\n",
      "Accuracy: 0.8238650709488461\n",
      "\n",
      "decision_tree trained.\n",
      "Accuracy: 0.8210372165833459\n",
      "\n",
      "knn trained.\n",
      "Accuracy: 0.20951371004393274\n",
      "\n",
      "gaussian_nb trained.\n",
      "Accuracy: 0.12710195424935616\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'logistic_regression': LogisticRegression(),\n",
    "    'random_forest': RandomForestClassifier(),\n",
    "    'decision_tree': DecisionTreeClassifier(),\n",
    "    'knn': KNeighborsClassifier(),\n",
    "    'gaussian_nb': GaussianNB(),\n",
    "}\n",
    "\n",
    "# train and test models\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    print(name + ' trained.')\n",
    "    y_pred = model.predict(X_test)\n",
    "    print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "\n",
    "The most accurate model was Random Forest, with an accuracy of ~82%. This may be due to its ability to predict psuedo-random data. Let's build a Random Forest model and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8249760137352926\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2054   76   70   68   68   62   56   49]\n",
      " [  56 2063   66   77   57   64   72   61]\n",
      " [  62   38 2055   56   67   80   44   76]\n",
      " [  70   54   56 2035   56   40   56   48]\n",
      " [  66   90   60   66 2018   64   57   66]\n",
      " [  55   62   61   52   68 2071   57   54]\n",
      " [  60   64   62   52   51   58 2039   50]\n",
      " [  66   82   67   64   66   76   65 2002]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.82      0.82      2503\n",
      "           1       0.82      0.82      0.82      2516\n",
      "           2       0.82      0.83      0.83      2478\n",
      "           3       0.82      0.84      0.83      2415\n",
      "           4       0.82      0.81      0.82      2487\n",
      "           5       0.82      0.84      0.83      2480\n",
      "           6       0.83      0.84      0.84      2436\n",
      "           7       0.83      0.80      0.82      2488\n",
      "\n",
      "    accuracy                           0.82     19803\n",
      "   macro avg       0.83      0.83      0.83     19803\n",
      "weighted avg       0.83      0.82      0.82     19803\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "# train model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# print accuracy, confusion matrix, and classification report\n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "print()\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print()\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Income (Remove Duplicates)\n",
    "\n",
    "What happens if we remove all duplicate data? Then our accuracy will be much worse. Let's see how much worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   file  country  gender  age  income\n",
      "0  3475       21       0    6       7\n",
      "1  4678       60       1    5       7\n",
      "2  4678       60       1    5       7\n",
      "3  2116      153       1    5       3\n",
      "4  2116      153       1    5       3\n",
      "(49500, 5)\n",
      "Accuracy: 0.12323232323232323\n",
      "\n",
      "Confusion Matrix:\n",
      "[[159 138 174 187 151 157 147 164]\n",
      " [153 159 161 152 163 142 142 137]\n",
      " [172 150 143 147 172 161 153 165]\n",
      " [158 153 164 133 145 144 152 135]\n",
      " [165 148 148 162 155 137 151 153]\n",
      " [154 153 165 155 153 179 144 158]\n",
      " [136 177 169 152 157 156 159 136]\n",
      " [166 166 183 174 130 149 144 133]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.13      0.12      0.13      1277\n",
      "           1       0.13      0.13      0.13      1209\n",
      "           2       0.11      0.11      0.11      1263\n",
      "           3       0.11      0.11      0.11      1184\n",
      "           4       0.13      0.13      0.13      1219\n",
      "           5       0.15      0.14      0.14      1261\n",
      "           6       0.13      0.13      0.13      1242\n",
      "           7       0.11      0.11      0.11      1245\n",
      "\n",
      "    accuracy                           0.12      9900\n",
      "   macro avg       0.12      0.12      0.12      9900\n",
      "weighted avg       0.12      0.12      0.12      9900\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = process_income_data()\n",
    "\n",
    "# remove all duplicates from dataframe\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# print the number of rows and columns\n",
    "print(df.shape)\n",
    "\n",
    "X_train, y_train, X_test, y_test = generate_train_test_split(df)\n",
    "\n",
    "# train random forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# train model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# print accuracy, confusion matrix, and classification report\n",
    "print('Accuracy: ' + str(accuracy_score(y_test, y_pred)))\n",
    "print()\n",
    "\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
